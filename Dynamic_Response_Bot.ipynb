{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCt6Q0XzwrJ",
        "outputId": "5f6bdda0-ad0c-4e0e-ee23-0a59aa9d4ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ivx9X3wNGCR",
        "outputId": "af43236b-817d-468d-a65b-9d3602285413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Downloading the tokenizer\n",
        "nltk.download('wordnet')  # Downloading the Dictionary\n",
        "nltk.download('omw-1.4')  # Downloading the WordNet interface\n",
        "nltk.download('stopwords')  # Downloading stopwords\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAy57RqGO86U"
      },
      "source": [
        "#GETTING INFORMATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRhaBQ90NlBQ"
      },
      "outputs": [],
      "source": [
        "def fetch_wikipedia_data(query):\n",
        "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data['extract']  # Return the summary/extract of the topic\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2CPjfmoQhEC"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Function to lemmatize tokens\n",
        "def LemToken(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "# Removing punctuation from the text\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# Normalizing the text by tokenizing, removing punctuation, and lemmatizing\n",
        "def LemNormalize(text):\n",
        "    return LemToken(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljUVOnbwROjb"
      },
      "outputs": [],
      "source": [
        "greet_inputs = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey')\n",
        "greet_responses = ('hi', 'hey', 'hi there', 'hello', 'I am glad! You are talking to me')\n",
        "\n",
        "# Greeting function\n",
        "def greet(sent):\n",
        "    for word in sent.split():\n",
        "        if word.lower() in greet_inputs:\n",
        "            return random.choice(greet_responses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzxgFZx7TwmI"
      },
      "source": [
        "#INTELLEGENCE OF CHATBOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1b4LinbTW30"
      },
      "outputs": [],
      "source": [
        "def fetch_info(topic):\n",
        "    try:\n",
        "        summary = wikipedia.summary(topic, sentences=2)  # Fetch a summary from Wikipedia\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        return \"There are multiple topics. Please be more specific.\"\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return \"I couldn't find information on that topic. Please try another one.\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjQxCh1-VhvD"
      },
      "source": [
        "#CHAT FLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8mmeEfpVjg_",
        "outputId": "e78abf1b-9f09-4809-ebc7-8fe920d91cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am a Learning Bot. Ask me anything! For ending the conversation, type 'bye'.\n",
            "france\n",
            "Bot:  France, officially the French Republic, is a country located primarily in Western Europe. Its overseas regions and territories include French Guiana in South America, Saint Pierre and Miquelon in the North Atlantic, the French West Indies, and many islands in Oceania and the Indian Ocean, giving it one of the largest discontiguous exclusive economic zones in the world.\n",
            "what is data science\n",
            "Bot:  Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data. \n",
            "Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).\n",
            "capital of india\n",
            "Bot:  The National Capital Region (NCR; Hindi: Rāṣṭrīya Rājadhānī Kṣētra) is a planning region centred upon the National Capital Territory (NCT) of Delhi in India. It encompasses Delhi and several districts surrounding it from the states of Haryana, Uttar Pradesh, and Rajasthan.\n",
            "thank you\n",
            "Bot: You are welcome\n"
          ]
        }
      ],
      "source": [
        "# Main function to run the chatbot\n",
        "def chatbot():\n",
        "    print(\"Hello! I am a Learning Bot. Ask me anything! For ending the conversation, type 'bye'.\")\n",
        "\n",
        "    flag = True\n",
        "    while flag:\n",
        "        user_response = input()\n",
        "        user_response = user_response.lower()\n",
        "\n",
        "        if user_response != 'bye':\n",
        "            if user_response == 'thanks' or user_response == 'thank you':\n",
        "                flag = False\n",
        "                print('Bot: You are welcome')\n",
        "            else:\n",
        "                if greet(user_response) is not None:\n",
        "                    print('Bot: ' + greet(user_response))\n",
        "                else:\n",
        "                    response = fetch_info(user_response)\n",
        "                    print('Bot: ', response)  # Response from Wikipedia\n",
        "        else:\n",
        "            flag = False\n",
        "            print('Bot: Goodbye! Take care')\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()\n",
        "Data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}